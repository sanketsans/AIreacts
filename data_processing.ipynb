{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "caffe_root = '~/caffe/' \n",
    "import sys\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "DEMO_DIR = './test/AgeGenderDeepLearning'\n",
    "\n",
    "categories = [ 'Angry' , 'Disgust' , 'Fear' , 'Happy'  , 'Neutral' ,  'Sad' , 'Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showimage(im):\n",
    "    if im.ndim == 3:\n",
    "        im = im[:, :, ::-1]\n",
    "    plt.set_cmap('jet')\n",
    "    plt.imshow(im,vmin=0, vmax=0.2)\n",
    "    \n",
    "\n",
    "def vis_square(data, padsize=1, padval=0):\n",
    "    data -= data.min()\n",
    "    data /= data.max()\n",
    "    \n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant', constant_values=(padval, padval))\n",
    "    \n",
    "    # tile the filters into an image\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    \n",
    "    showimage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_net_dir = 'VGG_S_rgb'\n",
    "# DEMO_DIR = '/home/sans/Videos/ai_reacts/test/AgeGenderDeepLearning/models'\n",
    "mean_filename=os.path.join(DEMO_DIR,cur_net_dir,'mean.binaryproto')\n",
    "proto_data = open(mean_filename, \"rb\").read()\n",
    "a = caffe.io.caffe_pb2.BlobProto.FromString(proto_data)\n",
    "mean  = caffe.io.blobproto_to_array(a)[0]\n",
    "\n",
    "net_pretrained = os.path.join(DEMO_DIR,cur_net_dir,'EmotiW_VGG_S.caffemodel')\n",
    "net_model_file = os.path.join(DEMO_DIR,cur_net_dir,'deploy.prototxt')\n",
    "VGG_S_Net = caffe.Classifier(net_model_file, net_pretrained,\n",
    "                       mean=mean,\n",
    "                       channel_swap=(2,1,0),\n",
    "                       raw_scale=255,\n",
    "                       image_dims=(256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcap = cv2.VideoCapture(0) # 0=camera\n",
    "vcopy = vcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vcopy.isOpened():\n",
    "    width = vcopy.get(3)\n",
    "    height = vcopy.get(4)\n",
    "#     frame = frame[240:700, 250:620]\n",
    "    print(width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1b9db40dc74b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG_S_Net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moversample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\rPredicted category is \\t {} '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if vcopy.grab():\n",
    "        flag, frame = vcopy.retrieve()\n",
    "#         frame = frame[240:700, 250:620] ## GoT frame part\n",
    "        frame = frame[50:230, 300:550] ## Human Face - when 3 ppl are present\n",
    "#         frame = frame[30:200, 120:300]\n",
    "#         frame = frame[0:100, 0:200]\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces = face_cascade.detectMultiScale(gray,\n",
    "                                        scaleFactor=1.3,\n",
    "                                        minNeighbors=3,\n",
    "                                        minSize=(30, 30))\n",
    "        if len(faces) > 0:\n",
    "            for (x,y,w,h) in faces:\n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "#                 frame = frame[y:y + h, x:x + w]\n",
    "\n",
    "#         tight = cv2.Canny(frame, 400, 470)z\n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#         contours, hierarchy = cv2.findContours(tight, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "#         cframe = cv2.drawContours(frame, contours, -1, (0,255,0), 3)\n",
    "        \n",
    "        if not flag:\n",
    "            continue\n",
    "        else:\n",
    "            cv2.imshow('video', frame) \n",
    "            input_image = caffe.io.load_image(os.path.join(DEMO_DIR,cur_net_dir,'demo_image.png'))\n",
    "            prediction = VGG_S_Net.predict([input_image],oversample=False)\n",
    "            print('\\rPredicted category is \\t {} '.format(categories[prediction.argmax()]), end=\"\")\n",
    "    if cv2.waitKey(33) == ord('a'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
